\documentclass[a4paper,openbib]{report}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsfonts,amssymb}
\usepackage{latexsym}
\usepackage{euscript}
\usepackage{enumerate}
\usepackage{graphics}
\usepackage[dvips]{graphicx}
\usepackage{geometry}
\usepackage{wrapfig}
\usepackage[colorlinks=true,allcolors=black]{hyperref}



\righthyphenmin=2

\usepackage[14pt]{extsizes}

\geometry{left=3cm}% левое поле
\geometry{right=1cm}% правое поле
\geometry{top=2cm}% верхнее поле
\geometry{bottom=2cm}% нижнее поле

\renewcommand{\baselinestretch}{1.3}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

\newcommand{\longcomment}[1]{}

\begin{document}
\clubpenalty=10000
\widowpenalty=10000
{\centering{
	{\large\bf Отчёт}
	\\
	{\bfпо проекту ``SciLexic'',}
	представляемому на стипендиальный конкурс группы компаний ``Информсвязь''
}}

Проект выполнила: Момот Е. А., студентка 3 курса Математического факультета ВГУ

\subsection*{Постановка проблемы}

Чтобы иметь возможность публиковаться в зарубежных научных изданиях (в частности, на arXiv.org), студентам приходится писать статьи на английском языке, 
зачастую не очень хорошо им знакомом. В процессе этого возникает множество проблем, и одна из них заключается в определении приемлемости 
употребления некоторых слов и выражений как в конкретных областях науки, так и в научных статьях вообще. В качестве решения студенты могут обратиться 
за помощью к более опытным товарищам или преподавателям, или же самостоятельно искать статьи со схожей тематикой и смотреть, что там употребляется, а 
что нет. Но это, как известно, долго, не всегда доступно, а также не всегда даёт результат.

В проекте представлен вариант автоматизации поиска ненаучной или не соответствующей конкретной области науки лексики путём ``разборки'' более 
качественных статей (найденных и загруженных пользователем) на слова, и, после такой же ``разборки'', сравнения лексики, примененной в статье 
пользователя, с лексикой всех более качественных статей.

Данная программа не предназначена для жёсткой оценки качества текста статьи, но она может указать места в тексте, на которые автору следует 
обратить внимание, и таким образом помочь ему сделать статью более грамотной.

Набор статей, по которому будет проводиться анализ текста, далее будем называть корпусом, как и принято говорить в лингвистике, а процесс 
``разборки'' текста --- созданием словаря текста.

\subsection*{Обзор аналогов}

Чтобы избежать орфографических ошибок, многие студенты пишут свои статьи в известных текстовых редакторах, таких как LibreOffice Writer или Microsoft Office Word.
Однако такие редакторы зачастую не могут указать на ошибки в математических терминах и на правильность их употребления в случае конкретной научной области. 

Например, некоторые термины, как ``line segment'' в геометрии и ``interval'' в функциональном анализе, на русском мы назовём одинаково --- ``отрезок''. 
Из-за этого неопытный человек может запутаться и применить, например, термин ``line segment'' в статье по функциональному анализу. Далее, собрав корпус 
из статей по функциональному анализу, написанных более опытными людьми и сравнив с ним свою статью, он получит список слов, которые есть в его статье, но 
отсутствуют в корпусе и увидит там что-то из ``line'' или ``segment''. Так станет ясно, что такие слова скорее всего не применяются 
в данной области науки.

Одним из распространенных форматов написания статей является .TeX, текст которого, кроме слов, содержит множество командных символов, которые сильно усложняют 
проверку такого текста на грамотность в обычном редакторе. Но этот формат легко преобразовывается в pdf и таким образом может быть проанализирован данной программой.

Также эти редакторы часто определяют как ошибки большинство математических и прочих научных обозначений, которые пишутся латинскими буквами, но не представляют 
собой самостоятельное слово в каком-либо языке. 

Тем же способом проводится профилактика ненаучной лексики, которая, напротив, никак не смущает распространенные текстовые редакторы. Например, для слова 
``исключить'' и ``throw out'', и ``exclude'' будет правильно, но первое --- ненаучно.

\subsection*{Извлечение текста из файлов}

В качестве основного формата для анализируемых текстов был выбран PDF, так как большинство других форматов легко в него преобразовать. В дальнейшем планируется 
сделать возможным загрузку и других распространенных форматов: .TeX, .odt, .rtf, .txt.

Для извлечения текста из pdf-файлов было решено использовать уже существующие библиотеки для работы с данным форматом. При выборе библиотеки возникало множество 
трудностей в связи с тем, что большинство их работает либо только в командной строке, либо только в браузере. При этом разные библиотеки получают разный результат 
при извлечении текста из файлов, из-за чего использование различных библиотек в браузере и командной строке приводит к несоответствиям в результатах работы программы 
с одним и тем же файлом.

В процессе решения этих проблем были произведены попытки модификации некоторых библиотек для командной строки, чтобы заставить их 
работать в браузере.

Одной из первых была испробована библиотека pdf-to-text \\( https://www.npmjs.com/package/pdf-to-text ). Она была выбрана из нескольких вариантов, которые есть в реестре пакетов NPM, 
за наиболее точное соответствие извлеченного текста оригинальному тексту документа. На её основе были созданы первые версии функций составления словарей и анализа текстов. 
Однако, в процессе сборки веб-интерфейса выяснилось, что функция этой библиотеки, отвечающая за извлечение текста из файла, запускала дочерний процесс (\verb|child_process|), 
что не поддерживается в браузере.

Следующей была библиотека pdf-parse \\( https://www.npmjs.com/package/pdf-parse ). Она не запускала дочерних процессов, а используемый в одном из основных файлов 
\verb|fs.readFileSync(path_to_pdf)| можно было легко заменить на \verb|FileReader().readAsArrayBuffer()|, после чего, по идее, проект должен был собраться 
с помощью browserify. Но browserify с задачей не справился, предположительно из-за нестандартного вызова функции \verb|require()| (аргумент был не константным 
строковым выражением, а суммой некоторых переменных). Тем не менее, в коде этой библиотеки нашлось более надежное решение, которое и стало окончательным.

Этим решением стала библиотека от разработчиков FireFox \\( https://github.com/mozilla/pdf.js ), которая без проблем запустилась в браузере, а затем и в командной 
строке. 

После того, как текст файла получен, для создания словаря нужно разделить его на слова. При этом существует несколько проблем, из-за которых тексту требуется 
предварительная обработка.
\begin{enumerate}
\item
 Переносы.
 После извлечения текста из файла слово с переносом в строке JavaScript выглядит примерно следующим образом: \verb|"сло-\nво"|. То есть такие слова нельзя выделять 
 так же, как целые, в них есть лишние символы. Чтобы ``склеить'' переносы, производится первая обработка текста с помощью функции \verb|str.replace(/-\s+/g,'')|. 
 Эта функция заменяет символьные последовательности, соответствующие регулярному выражению, переданному в качестве первого аргумента, на строку, переданную в 
 качестве второго аргумента. Так перенос заменяется пустой строкой --- ``склеивается''.
\item
 Лигатуры и диакритические знаки.
 Диакритические знаки в типографике --- элементы письменности, модифицирующие начертание знаков и обычно набираемые отдельно.
 Лигатура --- знак любой системы письма или фонетической транскрипции, образованный путём соединения двух и более графем (напр. ffi).
 Из этого следует, что в разных языках в текстах могут попадаться символы, состоящие на самом деле из нескольких символов. При этом могут встречаться различные 
 варианты написания одного и того же текста. Например, если в одном тексте использовались лигатуры, а в другом --- нет, одно и то же слово, получается, будет записано 
 разными символами и эти слова будут распознаны программой как разные. Примерно та же ситуация с диакритическими знаками: в одних случаях они могут быть написаны 
 отдельно от модифицируемых ими букв, но у некоторых символов в связке с диакритическими знаками существуют ``цельные'' версии (напр. ``ё''). Соответственно, 
 слова с ``цельными'' и ``раздельными'' символами также будут считаться разными.
 Отсюда появляется ещё один этап обработки --- нормализация, то есть приведение символов к одинаковому формату, метод \verb|str.normalize('NFKC')| (описан в спецификации Юникода). 
 Он выполняет каноническую декомпозицию, т.е. заменяет лигатуры соответствующими им последовательностями символов, а диакритические знаки компонует с соответствующими 
 им буквами, приводя таким образом все символы в текстах к единому формату.
\end{enumerate}
 Теперь, когда текст состоит из цельных слов и символов одного формата, можно разделить его на слова. Проблема этого процесса в том, что текст, как известно, состоит 
не только из, собственно, слов: научные статьи часто включают в себя множество формул и расчётов, а также общие для всех текстов знаки препинания. Так как встроенными 
методами можно было выбрать из текста только слова из латинских символов, нужно было создать то, что сможет отделить все буквенные символы от формул, знаков препинания 
и прочего. Для этого было сконструировано регулярное выражение, состоящее по большей части из диапазонов юникода, в которых находились символы, используемые в формулах. 
Также оно отдельно содержит некоторые распространенные символы, не попавшие ни в один из существующих диапазонов и не формирующих в совокупности новый диапазон.

\subsection*{Структура проекта}

Сперва проект был задуман как консольное приложение, но так как пользователям всё же проще обращаться с графическим интерфейсом, он был ``продлён'' в браузер.

На данный момент проект состоит из двух разделов, предназначенных для пользователей и раздела математической модели:
\begin{enumerate}
\item
 Web UI - графический веб-интерфейс;
\item
 CLI - интерфейс командной строки;
\item
 Common - набор функций для предварительной обработки и анализа текста
\end{enumerate}
 
Для сборки используется система grunt; использование основного кода и в браузере, и в командной строке обеспечивает browserify

Веб-версия проекта доступна по адресу https://scilexic.github.io/ .

Исходный код: https://github.com/Aisse-258/scilexic .
\subsection*{Математическая модель}

 Данная программа сверяет текст статьи с множеством других, в которых эти термины скорее всего использованы правильно и написаны без ошибок 
(или, возможно, с другой ошибкой, если это просто опечатка). Таким образом, если в анализируемом тексте термин будет написан неправильно, 
или будет использован тот, который обычно не используется в данной области исследований, он не попадется в корпусе и будет выведен в соответствующий список. 
Если в корпусе есть ошибки, они скорее всего будут встречаться в нем 1-2 раза. Из этого получается возможность ``поймать'' ошибки в тексте, 
которые совпадают с ошибками в корпусе --- список слов из текста, встречающихся в корпусе менее заданного $n$ раз.

Таким образом, результат сравнения выводится в виде двух списков ``потенциально опасных слов'', один 
из которых содержит все слова пользовательской статьи, не встречающихся в корпусе, а другой --- слова, редко встречающиеся в 
корпусе (1, 2, или любое заданное число раз), которые были найдены и в анализируемом тексте (возможно, опечатки).

В ближайшем будущем планируется анализировать сочетаемость слов в тексте и указывать пользователю на возможную несочетаемость слов. Анализ будет проводиться примерно 
следующим образом: если частота встречи в корпусе пары А Б существенно ниже, чем произведение частот А и Б (или даже ноль при ненулевых частотах А и Б по отдельности), 
то на употребление словосочетания А Б в анализируемом тексте нужно обратить внимание.

\subsection*{Альтернативная применимость}

Похожий подход может быть использован для уменьшения ложных срабатываний антиплагиата.
Для этого в качестве корпуса можно взять список цитируемой литературы, а часто встречающиеся словосочетания (напр. ``тогда и только тогда, когда'' в математике) 
рассматривать как единые лексические единицы (принимать за одно слово при подсчёте оригинальности).

\subsection*{Ограничения применимости}

\begin{enumerate}
\item
 Программа может работать только с языками, слова в которых разделены пробелами.
То есть, если статья написана, к примеру, на японском, программа примет за слова целые предложения или их части. Это связано с тем, что функция 
\verb|str.match(reg_word)| ищет совпадения с последовательностями буквенных символов. В языках с пробелами такие последовательности прерываются знаками препинания, 
и, собственно, пробелами. А если нет никаких разделителей между словами, несколько слов будут одной и той же последовательностью, т.е. приняты за одно слово.
\item
 Языки с большим количеством падежей труднее поддаются анализу.
Причина этого, как нетрудно догадаться, в большом количестве вариантов записи одного и того же слова. Это можно преодолеть, составив корпус из как можно большего 
количества статей, и проблемы наверняка будут у авторов статей, которые работают с малоизученными областями науки (им просто негде взять достаточное количество материала).
\item
 Греческий язык.
Символы этого языка нередко являются частями математических формул, и если при написании статьи на русском или английском понятно, что это формульные символы, 
то они будут причинять очевидный дискомфорт в случае, если греческий --- основной язык статьи.
\item
 Необходимость создания корпуса.
Пользователю предлагается самостоятельно создать корпус, по которому будет происходить анализ статьи. Если пользователь уже какое-то время работал по своей тематике, скорее всего 
уже будут некоторые статьи, которые можно использовать в качестве материала для корпуса, а вот кому-то только начинающему разбираться в теме, придется специально искать 
надежные статьи.
\end{enumerate}

\subsection*{Планы по развитию}

В дальнейшем планируется расширить функционал проекта для возможности получения более конкретных результатов анализа лексики текста на применимость в научных статьях.
В ближайшее время пбудут созданы методы анализа слов на сочетаемость для указания пользователю на потенциально несочитаемые слова.
Также планируется анализировать правильность порядка слов в словосочетаниях, для указания на возможный неправильный порядок слов.

В перспективе будет расширен список поддерживаемых форматов для обеспечения более быстрого и удобного пользования программой. Важность этого обусловлена тем, что статьи 
в архивах хранятся в разных форматах, а потребность преобразовывать их все к одному формату может стать отталкивающим фактором, особенно если статей много.

Локализация --- перевод веб-интерфейса на английский.

Указание контекста слов, редко встречающихся в корпусе и присутствующих в статье, для упрощения работы с результатом анализа текста и ускорения поиска слов, употребление 
которых потенциально ошибочно.

\end{document}

