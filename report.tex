\documentclass[a4paper,openbib]{report}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsfonts,amssymb}
\usepackage{latexsym}
\usepackage{euscript}
\usepackage{enumerate}
\usepackage{graphics}
\usepackage[dvips]{graphicx}
\usepackage{geometry}
\usepackage{wrapfig}
\usepackage[colorlinks=true,allcolors=black]{hyperref}



\righthyphenmin=2

\usepackage[14pt]{extsizes}

\geometry{left=3cm}% левое поле
\geometry{right=1cm}% правое поле
\geometry{top=2cm}% верхнее поле
\geometry{bottom=2cm}% нижнее поле

\renewcommand{\baselinestretch}{1.3}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

\newcommand{\longcomment}[1]{}

\begin{document}
\clubpenalty=10000
\widowpenalty=10000

\subsection*{Постановка проблемы}

Чтобы иметь возможность публиковаться в зарубежных научных изданиях (в частности, на arXiv.org), студентам приходится писать статьи на, английском языке, 
зачастую не очень хорошо им знакомом. В процессе этого возникает множество проблем, и одна из них заключается в определении приемлемости 
употребления некоторых слов и выражений как в конкретных областях науки, так и в научных статьях вообще. В качестве решения студенты могут обратиться 
за помощью к более опытным товарищам или преподавателям, или же самостоятельно искать статьи со схожей тематикой и смотреть, что там употребляется, а 
что нет. Но это, как известно, долго, не всегда доступно, а также не всегда даёт результат.
В проекте представлен вариант автоматизации поиска ненаучной или не соответствующей конкретной области науки лексики путём ``разборки'' более 
качественных статей (найденных и загруженных пользователем) на слова, и, после такой же ``разборки'', сравнения лексики, примененной в статье 
пользователя, с лексикой всех более качественных статей.

Лексический состав статей, по которому будет проводиться анализ текста далее будем называть корпусом, как и принято говорить в лингвистике, а процесс 
``разборки'' текста --- созданием словаря текста.

Сперва проект был задуман как консольное приложение, 

\subsection*{Обзор аналогов}

Чтобы избежать орфографических ошибок, многие студенты пишут свои статьи в известных текстовых редакторах, как LibreOffice Writer или Microsoft Office Word.
Однако такие редакторы зачастую не могут указать на ошибки в математических терминах и на правильность их употребления в случае конкретной научной области. 
Например, некоторые термины, как ``line segment'' в геометрии и ``interval'' в функциональном анализе на русском мы назовем одинаково --- ``отрезок''. 
Из-за этого неопытный человек может запутаться и применить, например, термин ``line segment'' в статье по функциональному анализу. Далее, собрав корпус 
из статей по функциональному анализу, написанных более опытными людьми и сравнив с ним свою статью он получит список слов, которые есть в его статье, но 
отсутствуют в корпусе и увидит там что-то из ``line'' или ``segment''. Так станет ясно, что такие слова скорее всего не применяются 
в данной области науки.
Также эти редакторы часто определяют как ошибки большинство математических и прочих научных обозначений, которые пишутся латинскими буквами но не представляют 
собой самостоятельное слово в каком-либо языке. 
Тем же способом проводится профилактика ненаучной лексики, которая, напротив, никак не смущает распространенные текстовые редакторы. Например, для слова 
``исключить'' и ``throw out'', и ``exclude'' будет правильно, но первое --- ненаучно.

\subsection*{Извлечение текста из файлов}

В качестве основного формата для анализируемых текстов был выбран PDF, так как большинство других форматов легко в него преобразовать. В дальнейшем планируется 
сделать возможным загрузку и других распространенных форматов, как .TeX, .odt, .rtf, .txt.

Для извлечения текста из pdf-файлов было решено использовать уже существующие библиотеки для работы с данным форматом. При выборе библиотеки возникало множество 
трудностей в связи с тем, что большинство их работает либо только в командной строке, либо только в браузере. При этом разные библиотеки получают разный результат 
при извлечении текста из файлов, из-за чего использование различных библиотек в браузере и командной строке приводит к несоответствиям в результатах работы программы 
с одним и тем же файлом.

В процессе решения этих проблем были произведены попытки модификации некоторых библиотек для командной строки, чтобы заставить их 
работать в браузере.

Одной из первых была испробована библиотека pdf-to-text ( https://www.npmjs.com/package/pdf-to-text ). Она была выбрана из нескольких вариантов, которые есть на npmjs.com 
за наиболее точное соответствие извлеченного текста оригинальному тексту документа. На её основе были созданы первые версии функций составления словарей и анализа текстов. 
Однако, в процессе сборки веб-интерфейса выяснилось, что функция этой библиотеки, отвечающая за извлечение текста из файла, запускала дочерний процесс (\verb|child_process|), 
что не поддерживается в браузере.

Следующей была библиотека pdf-parse ( https://www.npmjs.com/package/pdf-parse ). Она не запускала дочерних процессов, а используемый в одном из основных файлов 
\verb|fs.readFileSync(path-to-pdf)| можно было легко заменить на \verb|FileReader().readAsArrayBuffer()|, после чего, по идее, проект должен был собраться 
с помощью browserify. Но browserify с задачей не справился, предположительно из-за нестандартного применения скрипта \verb|require()| (аргумент был не константным 
строковым выражением, а суммой некоторых переменных). Тем не менее, в коде этой библиотеки нашлось более надежное решение, которое и стало окончательным.

Этим решением стала библиотека от разработчиков FireFox ( https://github.com/mozilla/pdf.js ), которая без проблем запустилась в браузере, а затем и в командной 
строке. 

После того, как текст файла получен, для создания словаря нужно разделить его на слова. При этом существует несколько проблем, из-за которых тексту требуется 
предварительная обработка.
\begin{enumerate}
\item
 Переносы.
 После извлечения текста из файла слово с переносом в строке JavaScript выглядит примерно следующим образом: \verb|''сло-\nво''|. То есть такие слова нельзя выделять 
 так же, как целые, в них есть лишние символы. Чтобы ``склеить'' переносы, производится первая обработка текста с помощью функции \verb|str.replace(/-\s+/g,'')|. 
 Эта функция заменяет символьные последовательности, соответствующие регулярному выражению, переданному в качестве первого аргумента, на строку, переданную в 
 качестве второго аргумента. Так перенос заменяется пустой строкой --- ``склеивается''.
\item
 Лигатуры и диакритические знаки.
 Диакритические знаки в типографике --- элементы письменности, модифицирующие начертание знаков и обычно набираемые отдельно.
 Лигатура --- знак любой системы письма или фонетической транскрипции, образованный путём соединения двух и более графем (напр. ffi).
 Из этого следует, что в разных языках в текстах могут попадаться символы, состоящие на самом деле из нескольких символов. При этом могут встречаться различные 
 варианты написания одного и того же текста. Например, если в одном тексте использовались лигатуры, а в другом --- нет, одно и то же слово, получается, будет записано 
 разными символами и эти слова будут распознаны программой как разные. Примерно та же ситуация с диакритическими знаками: в одних случаях они могут быть написаны 
 отдельно от модифицируемых ими букв, но у некоторых символов в связке с диакритическими знаками существуют ``цельные'' версии (напр. ``ё''). Соответственно, 
 слова с ``цельными'' и ``раздельными'' символами также будут считаться разными.
 Отсюда появляется ещё один этап обработки --- нормализация, то есть приведение символов к одинаковому формату, метод \verb|str.normalize('NFKC')|.
\end{enumerate}
Теперь, когда текст состоит из цельных слов и символов одного формата, можно разделить его на слова. Проблема этого процесса в том, что текст, как известно, состоит 
не только из, собственно, слов: научные статьи часто включают в себя множество формул и рассчетов, а также общие для всех текстов знаки препинания. Так как встроенными 
методами можно было выбрать из текста только слова из латинских символов, нужно было создать то, что сможет отделить все буквенные символы от формул, знаков препинания 
и прочего. Для этого было сконструировано регулярное выражение, состоящее по большей части из диапазонов юникода, в которых находились символы, используемые в формулах. 
Также оно отдельно содержит некоторые распространенные символы, не попавшие ни в один из существующих диапазонов и не формирующих в совокупности новый диапаон.

\subsection*{Структура проекта}

будет

\subsection*{Математическая модель}

Данная программа сверяет текст статьи с множеством других, в которых эти термины скорее всего использованы правильно и написаны без ошибок 
(или, возможно, с другой ошибкой, если это просто опечатка). Таким образом, если в анализируемом тексте термин будет написан неправильно, 
или будет использован тот, который обычно не используется в данной области исследований, он не попадется в корпусе и будет выведен в соответствующий список. 
Если в корпусе есть ошибки, они скорее всего будут встречаться в нем 1-2 раза. Из этого получается возможность ``поймать'' ошибки в тексте, 
которые совпадают с ошибками в корпусе --- список слов из текста, встречающихся в корпусе менее заданного $n$ раз.
Таким образом, результат сравнения выводится в виде двух списков ``потенциально опасных слов'', один 
из которых содержит все слова пользовательской статьи, не встречающихся в корпусе, а другой --- слова, редко встречающиеся в 
корпусе (1, 2, или любое заданное число раз), которые были найдены и в анализируемом тексте (возможно, опечатки).

\subsection*{Планы по развитию}

\subsection*{Ограничения применимости}

Ограничения применимости можно условно поделить на две группы. Первая связана с тем, что научные статьи пишутся на разных языках, у каждого из которых есть свои 
особенности.
\begin{enumerate}
\item
 Программа может рабоать только с языками, слова в которых разделены пробелами.
\end{enumerate}

\end{document}

